package tokenizer

import (
	"context"
	"io"
	"log/slog"
	"testing"

	"github.com/ajitpratap0/GoSQLX/pkg/models"
	"github.com/ajitpratap0/GoSQLX/pkg/sql/keywords"
)

// TestBacktickIdentifiers tests MySQL-style backtick identifiers
func TestBacktickIdentifiers(t *testing.T) {
	tests := []struct {
		name      string
		input     string
		wantToken models.TokenType
		wantValue string
		wantErr   bool
	}{
		{
			name:      "Simple backtick identifier",
			input:     "`user_id`",
			wantToken: models.TokenTypeIdentifier,
			wantValue: "user_id",
			wantErr:   false,
		},
		{
			name:      "Backtick identifier with spaces",
			input:     "`user name`",
			wantToken: models.TokenTypeIdentifier,
			wantValue: "user name",
			wantErr:   false,
		},
		{
			name:      "Backtick identifier with special characters",
			input:     "`user-id@domain`",
			wantToken: models.TokenTypeIdentifier,
			wantValue: "user-id@domain",
			wantErr:   false,
		},
		{
			name:      "Escaped backtick in identifier",
			input:     "`user``id`",
			wantToken: models.TokenTypeIdentifier,
			wantValue: "user`id",
			wantErr:   false,
		},
		{
			name:      "Backtick identifier with newline",
			input:     "`user\nid`",
			wantToken: models.TokenTypeIdentifier,
			wantValue: "user\nid",
			wantErr:   false,
		},
		{
			name:      "Unterminated backtick identifier",
			input:     "`unterminated",
			wantToken: models.TokenTypeUnknown,
			wantValue: "",
			wantErr:   true,
		},
		{
			name:      "Empty backtick identifier",
			input:     "``",
			wantToken: models.TokenTypeIdentifier,
			wantValue: "",
			wantErr:   false,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			tkz := GetTokenizer()
			defer PutTokenizer(tkz)

			tokens, err := tkz.Tokenize([]byte(tt.input))

			if tt.wantErr {
				if err == nil {
					t.Error("Expected error, got nil")
				}
				return
			}

			if err != nil {
				t.Errorf("Unexpected error: %v", err)
				return
			}

			if len(tokens) == 0 {
				t.Error("Expected tokens, got empty result")
				return
			}

			if tokens[0].Token.Type != tt.wantToken {
				t.Errorf("Expected token type %v, got %v", tt.wantToken, tokens[0].Token.Type)
			}

			if tokens[0].Token.Value != tt.wantValue {
				t.Errorf("Expected token value %q, got %q", tt.wantValue, tokens[0].Token.Value)
			}
		})
	}
}

// TestTripleQuotedStrings tests triple-quoted string literals
func TestTripleQuotedStrings(t *testing.T) {
	t.Skip("FEATURE NOT IMPLEMENTED: Triple-quoted strings are planned but not yet fully supported. See TASKS.md for roadmap.")

	tests := []struct {
		name      string
		input     string
		wantToken models.TokenType
		wantValue string
		wantErr   bool
	}{
		{
			name:      "Triple single-quoted string",
			input:     "'''hello world'''",
			wantToken: models.TokenTypeTripleSingleQuotedString,
			wantValue: "hello world",
			wantErr:   false,
		},
		{
			name:      "Triple double-quoted string",
			input:     `"""hello world"""`,
			wantToken: models.TokenTypeTripleDoubleQuotedString,
			wantValue: "hello world",
			wantErr:   false,
		},
		{
			name:      "Triple-quoted with newlines",
			input:     "'''line 1\nline 2\nline 3'''",
			wantToken: models.TokenTypeTripleSingleQuotedString,
			wantValue: "line 1\nline 2\nline 3",
			wantErr:   false,
		},
		{
			name:      "Triple-quoted with embedded quotes",
			input:     `'''She said "Hello" and 'Goodbye''''`,
			wantToken: models.TokenTypeTripleSingleQuotedString,
			wantValue: `She said "Hello" and 'Goodbye'`,
			wantErr:   false,
		},
		{
			name:      "Empty triple-quoted string",
			input:     "''''''",
			wantToken: models.TokenTypeTripleSingleQuotedString,
			wantValue: "",
			wantErr:   false,
		},
		{
			name:      "Unterminated triple-quoted string",
			input:     "'''unterminated",
			wantToken: models.TokenTypeUnknown,
			wantValue: "",
			wantErr:   true,
		},
		{
			name:      "Triple-quoted multiline SQL",
			input:     `"""SELECT * FROM users WHERE id = 1"""`,
			wantToken: models.TokenTypeTripleDoubleQuotedString,
			wantValue: "SELECT * FROM users WHERE id = 1",
			wantErr:   false,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			tkz := GetTokenizer()
			defer PutTokenizer(tkz)

			tokens, err := tkz.Tokenize([]byte(tt.input))

			if tt.wantErr {
				if err == nil {
					t.Error("Expected error, got nil")
				}
				return
			}

			if err != nil {
				t.Errorf("Unexpected error: %v", err)
				return
			}

			if len(tokens) == 0 {
				t.Error("Expected tokens, got empty result")
				return
			}

			if tokens[0].Token.Type != tt.wantToken {
				t.Errorf("Expected token type %v, got %v", tt.wantToken, tokens[0].Token.Type)
			}

			if tokens[0].Token.Value != tt.wantValue {
				t.Errorf("Expected token value %q, got %q", tt.wantValue, tokens[0].Token.Value)
			}
		})
	}
}

// TestEscapeSequences tests escape sequences in string literals
// NOTE: This test documents intended functionality; some escape sequences are not yet fully implemented
func TestEscapeSequences(t *testing.T) {
	t.Skip("FEATURE NOT FULLY IMPLEMENTED: Some escape sequences (double-quote in double-quoted strings, backtick escapes) are not yet supported. See TASKS.md for roadmap.")

	tests := []struct {
		name      string
		input     string
		wantValue string
		wantErr   bool
	}{
		{
			name:      "Newline escape",
			input:     `'hello\nworld'`,
			wantValue: "hello\nworld",
			wantErr:   false,
		},
		{
			name:      "Tab escape",
			input:     `'hello\tworld'`,
			wantValue: "hello\tworld",
			wantErr:   false,
		},
		{
			name:      "Carriage return escape",
			input:     `'hello\rworld'`,
			wantValue: "hello\rworld",
			wantErr:   false,
		},
		{
			name:      "Backslash escape",
			input:     `'C:\\Program Files\\'`,
			wantValue: `C:\Program Files\`,
			wantErr:   false,
		},
		{
			name:      "Single quote escape",
			input:     `'It\'s working'`,
			wantValue: "It's working",
			wantErr:   false,
		},
		{
			name:      "Double quote escape in double-quoted string",
			input:     `"She said \"Hello\""`,
			wantValue: `She said "Hello"`,
			wantErr:   false,
		},
		{
			name:      "Backtick escape",
			input:     "`name\\`escaped`",
			wantValue: "name`escaped",
			wantErr:   false,
		},
		{
			name:      "Multiple escape sequences",
			input:     `'Line 1\nLine 2\tTabbed\nLine 3\\'`,
			wantValue: "Line 1\nLine 2\tTabbed\nLine 3\\",
			wantErr:   false,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			tkz := GetTokenizer()
			defer PutTokenizer(tkz)

			tokens, err := tkz.Tokenize([]byte(tt.input))

			if tt.wantErr {
				if err == nil {
					t.Error("Expected error, got nil")
				}
				return
			}

			if err != nil {
				t.Errorf("Unexpected error: %v", err)
				return
			}

			if len(tokens) == 0 {
				t.Error("Expected tokens, got empty result")
				return
			}

			if tokens[0].Token.Value != tt.wantValue {
				t.Errorf("Expected value %q, got %q", tt.wantValue, tokens[0].Token.Value)
			}
		})
	}
}

// TestNumberFormats tests various number formats
func TestNumberFormats(t *testing.T) {
	tests := []struct {
		name      string
		input     string
		wantValue string
		wantErr   bool
	}{
		{
			name:      "Integer",
			input:     "42",
			wantValue: "42",
			wantErr:   false,
		},
		{
			name:      "Decimal",
			input:     "3.14159",
			wantValue: "3.14159",
			wantErr:   false,
		},
		{
			name:      "Scientific notation lowercase e",
			input:     "1.23e4",
			wantValue: "1.23e4",
			wantErr:   false,
		},
		{
			name:      "Scientific notation uppercase E",
			input:     "1.23E4",
			wantValue: "1.23E4",
			wantErr:   false,
		},
		{
			name:      "Scientific notation with positive exponent",
			input:     "1.23e+4",
			wantValue: "1.23e+4",
			wantErr:   false,
		},
		{
			name:      "Scientific notation with negative exponent",
			input:     "1.23e-4",
			wantValue: "1.23e-4",
			wantErr:   false,
		},
		{
			name:      "Very small decimal",
			input:     "0.0001",
			wantValue: "0.0001",
			wantErr:   false,
		},
		{
			name:      "Very large number",
			input:     "999999999",
			wantValue: "999999999",
			wantErr:   false,
		},
		{
			name:      "Zero",
			input:     "0",
			wantValue: "0",
			wantErr:   false,
		},
		{
			name:      "Decimal zero",
			input:     "0.0",
			wantValue: "0.0",
			wantErr:   false,
		},
		{
			name:      "Scientific zero",
			input:     "0e0",
			wantValue: "0e0",
			wantErr:   false,
		},
		{
			name:      "Invalid decimal - no digit after point",
			input:     "123.",
			wantValue: "",
			wantErr:   true,
		},
		{
			name:      "Invalid scientific - no digit in exponent",
			input:     "123e",
			wantValue: "",
			wantErr:   true,
		},
		{
			name:      "Invalid scientific - sign without digit",
			input:     "123e+",
			wantValue: "",
			wantErr:   true,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			tkz := GetTokenizer()
			defer PutTokenizer(tkz)

			tokens, err := tkz.Tokenize([]byte(tt.input))

			if tt.wantErr {
				if err == nil {
					t.Error("Expected error, got nil")
				}
				return
			}

			if err != nil {
				t.Errorf("Unexpected error: %v", err)
				return
			}

			if len(tokens) == 0 {
				t.Error("Expected tokens, got empty result")
				return
			}

			if tokens[0].Token.Type != models.TokenTypeNumber {
				t.Errorf("Expected NUMBER token, got %v", tokens[0].Token.Type)
			}

			if tokens[0].Token.Value != tt.wantValue {
				t.Errorf("Expected value %q, got %q", tt.wantValue, tokens[0].Token.Value)
			}
		})
	}
}

// TestOperatorPunctuation tests operator and punctuation tokenization
// NOTE: This test expects generic TokenTypeOperator but tokenizer returns specific types (EQ, LT, GT, etc.)
func TestOperatorPunctuation(t *testing.T) {
	t.Skip("TEST DESIGN ISSUE: This test expects generic TokenTypeOperator but tokenizer correctly returns specific types (EQ, LT, GT, PLUS, etc.). Needs test redesign to match actual tokenizer behavior.")

	tests := []struct {
		name       string
		input      string
		wantTokens []models.TokenType
	}{
		{
			name:       "Assignment operator",
			input:      "x = 1",
			wantTokens: []models.TokenType{models.TokenTypeIdentifier, models.TokenTypeOperator, models.TokenTypeNumber},
		},
		{
			name:       "Comparison operators",
			input:      "< > <= >= != <>",
			wantTokens: []models.TokenType{models.TokenTypeOperator, models.TokenTypeOperator, models.TokenTypeOperator, models.TokenTypeOperator, models.TokenTypeOperator, models.TokenTypeOperator},
		},
		{
			name:       "Arithmetic operators",
			input:      "+ - * / %",
			wantTokens: []models.TokenType{models.TokenTypeOperator, models.TokenTypeOperator, models.TokenTypeOperator, models.TokenTypeOperator, models.TokenTypeOperator},
		},
		{
			name:       "Parentheses",
			input:      "( )",
			wantTokens: []models.TokenType{models.TokenTypeLParen, models.TokenTypeRParen},
		},
		{
			name:       "Brackets",
			input:      "[ ]",
			wantTokens: []models.TokenType{models.TokenTypeLBracket, models.TokenTypeRBracket},
		},
		{
			name:       "Comma and semicolon",
			input:      ", ;",
			wantTokens: []models.TokenType{models.TokenTypeComma, models.TokenTypeSemicolon},
		},
		{
			name:       "Dot notation",
			input:      "schema.table.column",
			wantTokens: []models.TokenType{models.TokenTypeIdentifier, models.TokenTypePeriod, models.TokenTypeIdentifier, models.TokenTypePeriod, models.TokenTypeIdentifier},
		},
		{
			name:       "Double colon (PostgreSQL)",
			input:      "value::INT",
			wantTokens: []models.TokenType{models.TokenTypeIdentifier, models.TokenTypeDoubleColon, models.TokenTypeIdentifier},
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			tkz := GetTokenizer()
			defer PutTokenizer(tkz)

			tokens, err := tkz.Tokenize([]byte(tt.input))

			if err != nil {
				t.Errorf("Unexpected error: %v", err)
				return
			}

			// Filter out whitespace and EOF tokens
			var nonWhitespace []models.TokenWithSpan
			for _, tok := range tokens {
				if tok.Token.Type != models.TokenTypeWhitespace && tok.Token.Type != models.TokenTypeEOF {
					nonWhitespace = append(nonWhitespace, tok)
				}
			}

			if len(nonWhitespace) != len(tt.wantTokens) {
				t.Errorf("Expected %d non-whitespace tokens, got %d", len(tt.wantTokens), len(nonWhitespace))
				for i, tok := range nonWhitespace {
					t.Logf("  Token %d: type=%v, value=%q", i, tok.Token.Type, tok.Token.Value)
				}
				return
			}

			for i, wantType := range tt.wantTokens {
				if nonWhitespace[i].Token.Type != wantType {
					t.Errorf("Token %d: expected type %v, got %v (value: %q)", i, wantType, nonWhitespace[i].Token.Type, nonWhitespace[i].Token.Value)
				}
			}
		})
	}
}

// TestQuotedIdentifiers tests double-quoted identifiers
func TestQuotedIdentifiers(t *testing.T) {
	tests := []struct {
		name      string
		input     string
		wantValue string
		wantErr   bool
	}{
		{
			name:      "Simple quoted identifier",
			input:     `"user_id"`,
			wantValue: "user_id",
			wantErr:   false,
		},
		{
			name:      "Quoted identifier with spaces",
			input:     `"user name"`,
			wantValue: "user name",
			wantErr:   false,
		},
		{
			name:      "Quoted identifier with special chars",
			input:     `"user-id@domain"`,
			wantValue: "user-id@domain",
			wantErr:   false,
		},
		{
			name:      "Empty quoted identifier",
			input:     `""`,
			wantValue: "",
			wantErr:   false,
		},
		{
			name:      "Unterminated quoted identifier",
			input:     `"unterminated`,
			wantValue: "",
			wantErr:   true,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			tkz := GetTokenizer()
			defer PutTokenizer(tkz)

			tokens, err := tkz.Tokenize([]byte(tt.input))

			if tt.wantErr {
				if err == nil {
					t.Error("Expected error, got nil")
				}
				return
			}

			if err != nil {
				t.Errorf("Unexpected error: %v", err)
				return
			}

			if len(tokens) == 0 {
				t.Error("Expected tokens, got empty result")
				return
			}

			// Quoted identifiers may be treated as strings or identifiers depending on dialect
			if tokens[0].Token.Value != tt.wantValue {
				t.Errorf("Expected value %q, got %q", tt.wantValue, tokens[0].Token.Value)
			}
		})
	}
}

// TestNewWithKeywords tests the NewWithKeywords constructor
func TestNewWithKeywords(t *testing.T) {
	// Create custom keyword set
	customKeywords := keywords.NewKeywords()

	// Create tokenizer with custom keywords
	tkz, err := NewWithKeywords(customKeywords)
	if err != nil {
		t.Errorf("Unexpected error: %v", err)
		return
	}
	if tkz == nil {
		t.Error("Expected tokenizer, got nil")
		return
	}

	// Test that it can tokenize
	input := "SELECT * FROM users"
	tokens, err := tkz.Tokenize([]byte(input))
	if err != nil {
		t.Errorf("Unexpected error: %v", err)
	}

	if len(tokens) == 0 {
		t.Error("Expected tokens, got empty result")
	}

	// Verify SELECT is recognized (keyword or specific SELECT token type)
	if tokens[0].Token.Value != "SELECT" {
		t.Errorf("Expected first token to be SELECT, got %v", tokens[0].Token.Value)
	}
}

// TestSetLogger tests the slog-based logger functionality
func TestSetLogger(t *testing.T) {
	tkz := GetTokenizer()
	defer PutTokenizer(tkz)

	// Set a debug-level slog logger
	logger := slog.New(slog.NewTextHandler(io.Discard, &slog.HandlerOptions{Level: slog.LevelDebug}))
	tkz.SetLogger(logger)

	// Test that tokenization still works with logger set
	input := "SELECT * FROM users"
	tokens, err := tkz.Tokenize([]byte(input))
	if err != nil {
		t.Errorf("Unexpected error: %v", err)
	}

	if len(tokens) == 0 {
		t.Error("Expected tokens, got empty result")
	}

	// Set logger to nil (should also work)
	tkz.SetLogger(nil)

	// Test again
	tokens, err = tkz.Tokenize([]byte(input))
	if err != nil {
		t.Errorf("Unexpected error: %v", err)
	}

	if len(tokens) == 0 {
		t.Error("Expected tokens, got empty result")
	}
}

// TestUTF8Positioning tests UTF-8 multi-byte character positioning
func TestUTF8Positioning(t *testing.T) {
	tests := []struct {
		name  string
		input string
		want  int // number of tokens expected
	}{
		{
			name:  "Chinese characters in string",
			input: "'‰Ω†Â•Ω‰∏ñÁïå'",
			want:  1,
		},
		{
			name:  "Emoji in string",
			input: "'Hello üëã World üåç'",
			want:  1,
		},
		{
			name:  "Japanese characters",
			input: "'„Åì„Çì„Å´„Å°„ÅØ'",
			want:  1,
		},
		{
			name:  "Korean characters",
			input: "'ÏïàÎÖïÌïòÏÑ∏Ïöî'",
			want:  1,
		},
		{
			name:  "Arabic characters",
			input: "'ŸÖÿ±ÿ≠ÿ®ÿß'",
			want:  1,
		},
		{
			name:  "Mixed UTF-8 and ASCII",
			input: "SELECT 'üî•' FROM users WHERE name = '‰∏ñÁïå'",
			want:  7, // SELECT, string, FROM, users, WHERE, name, =, string
		},
		{
			name:  "Emoji in identifier (backtick)",
			input: "`table_üî•`",
			want:  1,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			tkz := GetTokenizer()
			defer PutTokenizer(tkz)

			tokens, err := tkz.Tokenize([]byte(tt.input))
			if err != nil {
				t.Errorf("Unexpected error: %v", err)
				return
			}

			if len(tokens) < tt.want {
				t.Errorf("Expected at least %d tokens, got %d", tt.want, len(tokens))
			}
		})
	}
}

// TestTokenizeContext tests context-aware tokenization
func TestTokenizeContext(t *testing.T) {
	tests := []struct {
		name    string
		input   string
		wantErr bool
	}{
		{
			name:    "Simple SELECT with context",
			input:   "SELECT * FROM users",
			wantErr: false,
		},
		{
			name:    "Complex query with context",
			input:   "SELECT u.id, u.name FROM users u WHERE u.active = true",
			wantErr: false,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			tkz := GetTokenizer()
			defer PutTokenizer(tkz)

			ctx := context.Background()
			tokens, err := tkz.TokenizeContext(ctx, []byte(tt.input))

			if tt.wantErr && err == nil {
				t.Error("Expected error, got nil")
			}

			if !tt.wantErr && err != nil {
				t.Errorf("Unexpected error: %v", err)
			}

			if !tt.wantErr && len(tokens) == 0 {
				t.Error("Expected tokens, got empty result")
			}
		})
	}
}

// TestTokenizeCancellation tests tokenization with cancelled context
func TestTokenizeCancellation(t *testing.T) {
	tkz := GetTokenizer()
	defer PutTokenizer(tkz)

	// Create a cancelled context
	ctx, cancel := context.WithCancel(context.Background())
	cancel() // Cancel immediately

	// This should work even with cancelled context for short input
	input := "SELECT"
	tokens, err := tkz.TokenizeContext(ctx, []byte(input))

	// Either succeeds (if tokenization was fast) or fails with context error
	if err != nil && err != context.Canceled {
		// If we get an error, it should be context.Canceled
		t.Logf("Tokenization was cancelled as expected: %v", err)
	}

	if err == nil && len(tokens) == 0 {
		t.Error("Expected tokens or error, got neither")
	}
}

// TestPostgreSQLRegexOperators tests PostgreSQL regex matching operators
// Issue #190: Support PostgreSQL regular expression operators (~, ~*, !~, !~*)
func TestPostgreSQLRegexOperators(t *testing.T) {
	tests := []struct {
		name      string
		input     string
		wantToken models.TokenType
		wantValue string
		wantErr   bool
	}{
		{
			name:      "Tilde operator - case-sensitive regex match",
			input:     "~",
			wantToken: models.TokenTypeTilde,
			wantValue: "~",
			wantErr:   false,
		},
		{
			name:      "Tilde-asterisk operator - case-insensitive regex match",
			input:     "~*",
			wantToken: models.TokenTypeTildeAsterisk,
			wantValue: "~*",
			wantErr:   false,
		},
		{
			name:      "Exclamation-tilde operator - case-sensitive regex non-match",
			input:     "!~",
			wantToken: models.TokenTypeExclamationMarkTilde,
			wantValue: "!~",
			wantErr:   false,
		},
		{
			name:      "Exclamation-tilde-asterisk operator - case-insensitive regex non-match",
			input:     "!~*",
			wantToken: models.TokenTypeExclamationMarkTildeAsterisk,
			wantValue: "!~*",
			wantErr:   false,
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			tkz := GetTokenizer()
			defer PutTokenizer(tkz)

			tokens, err := tkz.Tokenize([]byte(tt.input))

			if tt.wantErr {
				if err == nil {
					t.Error("Expected error, got nil")
				}
				return
			}

			if err != nil {
				t.Errorf("Unexpected error: %v", err)
				return
			}

			if len(tokens) < 1 {
				t.Error("Expected at least one token, got empty result")
				return
			}

			if tokens[0].Token.Type != tt.wantToken {
				t.Errorf("Expected token type %v, got %v", tt.wantToken, tokens[0].Token.Type)
			}

			if tokens[0].Token.Value != tt.wantValue {
				t.Errorf("Expected token value %q, got %q", tt.wantValue, tokens[0].Token.Value)
			}
		})
	}
}

// TestPostgreSQLRegexOperatorsInSQL tests regex operators in SQL context
func TestPostgreSQLRegexOperatorsInSQL(t *testing.T) {
	tests := []struct {
		name        string
		input       string
		wantTokens  []models.TokenType
		description string
	}{
		{
			name:  "Case-sensitive regex match",
			input: "SELECT * FROM users WHERE name ~ '^J.*'",
			wantTokens: []models.TokenType{
				models.TokenTypeSelect,
				models.TokenTypeMul,
				models.TokenTypeFrom,
				models.TokenTypeIdentifier,
				models.TokenTypeWhere,
				models.TokenTypeIdentifier,
				models.TokenTypeTilde,
				models.TokenTypeSingleQuotedString,
			},
			description: "Simple regex match with ~ operator",
		},
		{
			name:  "Case-insensitive regex match",
			input: "SELECT * FROM products WHERE description ~* 'sale|discount'",
			wantTokens: []models.TokenType{
				models.TokenTypeSelect,
				models.TokenTypeMul,
				models.TokenTypeFrom,
				models.TokenTypeIdentifier,
				models.TokenTypeWhere,
				models.TokenTypeIdentifier,
				models.TokenTypeTildeAsterisk,
				models.TokenTypeSingleQuotedString,
			},
			description: "Regex match with ~* operator for case-insensitive search",
		},
		{
			name:  "Case-sensitive regex non-match",
			input: "SELECT * FROM logs WHERE message !~ 'DEBUG'",
			wantTokens: []models.TokenType{
				models.TokenTypeSelect,
				models.TokenTypeMul,
				models.TokenTypeFrom,
				models.TokenTypeIdentifier,
				models.TokenTypeWhere,
				models.TokenTypeIdentifier,
				models.TokenTypeExclamationMarkTilde,
				models.TokenTypeSingleQuotedString,
			},
			description: "Regex non-match with !~ operator",
		},
		{
			name:  "Case-insensitive regex non-match",
			input: "SELECT * FROM emails WHERE subject !~* 'spam'",
			wantTokens: []models.TokenType{
				models.TokenTypeSelect,
				models.TokenTypeMul,
				models.TokenTypeFrom,
				models.TokenTypeIdentifier,
				models.TokenTypeWhere,
				models.TokenTypeIdentifier,
				models.TokenTypeExclamationMarkTildeAsterisk,
				models.TokenTypeSingleQuotedString,
			},
			description: "Regex non-match with !~* operator for case-insensitive search",
		},
	}

	for _, tt := range tests {
		t.Run(tt.name, func(t *testing.T) {
			tkz := GetTokenizer()
			defer PutTokenizer(tkz)

			tokens, err := tkz.Tokenize([]byte(tt.input))
			if err != nil {
				t.Fatalf("Unexpected tokenization error: %v", err)
			}

			// Filter out EOF token for comparison
			var actualTokens []models.TokenType
			for _, tok := range tokens {
				if tok.Token.Type != models.TokenTypeEOF {
					actualTokens = append(actualTokens, tok.Token.Type)
				}
			}

			if len(actualTokens) != len(tt.wantTokens) {
				t.Errorf("Expected %d tokens, got %d", len(tt.wantTokens), len(actualTokens))
				t.Logf("Description: %s", tt.description)
				t.Logf("Expected: %v", tt.wantTokens)
				t.Logf("Actual:   %v", actualTokens)
				return
			}

			for i, want := range tt.wantTokens {
				if actualTokens[i] != want {
					t.Errorf("Token %d: expected %v, got %v", i, want, actualTokens[i])
					t.Logf("Description: %s", tt.description)
				}
			}
		})
	}
}
